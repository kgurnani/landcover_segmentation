{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9ffe6-2106-4e0e-bac1-4984f88650b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from torchmetrics.classification import MulticlassJaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93df827-0e31-479c-8fa0-f506d92eee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdc313-79b7-4192-bf50-e552608ff2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579539d1-52fc-4c36-b2fd-15b9293b6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNet\n",
    "        for feature in features:\n",
    "            self.downs.append(self._double_conv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNet\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(self._double_conv(feature * 2, feature))\n",
    "\n",
    "        self.bottleneck = self._double_conv(features[-1], features[-1] * 2)\n",
    "        self.transformer = TransformerBlock(features[-1] * 2, num_heads=8)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def _double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Down part\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Apply transformer to the bottleneck\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.flatten(2).permute(2, 0, 1)  # (H*W, B, C)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 2, 0).view(b, c, h, w)\n",
    "\n",
    "        # Up part\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9b385-295b-4780-adaf-f22ef83eb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taken from https://stackoverflow.com/questions/43884463/how-to-convert-rgb-image-to-one-hot-encoded-3d-array-based-on-color-using-numpy\n",
    "\n",
    "# color_dict = {\n",
    "#         0: (0, 0, 0),\n",
    "#         1: (1, 1, 1),\n",
    "#         2: (2, 2, 2),\n",
    "#         3: (3, 3, 3),\n",
    "#         4: (4, 4, 4),\n",
    "# }\n",
    "\n",
    "\n",
    "# def rgb_to_onehot(rgb_arr, color_dict):\n",
    "#     num_classes = len(color_dict)\n",
    "#     shape = rgb_arr.shape[:2]+(num_classes,)\n",
    "#     arr = np.zeros( shape, dtype=np.int8 )\n",
    "#     for i, cls in enumerate(color_dict):\n",
    "#         arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n",
    "#     return arr\n",
    "\n",
    "\n",
    "# def onehot_to_rgb(onehot, color_dict):\n",
    "#     single_layer = np.argmax(onehot, axis=-1)\n",
    "#     output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "#     for k in color_dict.keys():\n",
    "#         output[single_layer==k] = color_dict[k]\n",
    "#     return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813388bd-e490-403e-8986-4a2930874bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/hubutui/DiceLoss-PyTorch/blob/master/loss.py\n",
    "def make_one_hot(input, num_classes):\n",
    "    \"\"\"Convert class index tensor to one hot encoding tensor.\n",
    "\n",
    "    Args:\n",
    "         input: A tensor of shape [N, 1, *]\n",
    "         num_classes: An int of number of class\n",
    "    Returns:\n",
    "        A tensor of shape [N, num_classes, *]\n",
    "    \"\"\"\n",
    "    shape = np.array(input.shape)\n",
    "    shape[1] = num_classes\n",
    "    shape = tuple(shape)\n",
    "    result = torch.zeros(shape)\n",
    "    result = result.scatter_(1, input.cpu(), 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss of binary class\n",
    "    Args:\n",
    "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
    "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    Raise:\n",
    "        Exception if unexpected reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=0.00001, p=2, reduction='mean'):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
    "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
    "        target = target.contiguous().view(target.shape[0], -1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
    "\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss, need one hot encode input\n",
    "    Args:\n",
    "        weight: An array of shape [num_classes,]\n",
    "        ignore_index: class index to ignore\n",
    "        predict: A tensor of shape [N, C, *]\n",
    "        target: A tensor of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Return:\n",
    "        same as BinaryDiceLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=None, **kwargs):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape == target.shape, 'predict & target shape do not match'\n",
    "        dice = BinaryDiceLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "\n",
    "        for i in range(target.shape[1]):\n",
    "            if i != self.ignore_index:\n",
    "                dice_loss = dice(predict[:, i], target[:, i])\n",
    "                if self.weight is not None:\n",
    "                    assert self.weight.shape[0] == target.shape[1], \\\n",
    "                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n",
    "                    dice_loss *= self.weights[i]\n",
    "                total_loss += dice_loss\n",
    "\n",
    "        return total_loss/target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ee399-76d4-4867-8acd-a25e40881636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
    "\n",
    "# def dice_coef(y_true, y_pred, smooth):\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "#     return dice\n",
    "\n",
    "# def dice_coef_loss(y_true, y_pred, smooth):\n",
    "#     return 1 - dice_coef(y_true, y_pred, smooth)\n",
    "\n",
    "# def dice_coef_multilabel(y_true, y_pred, M, smooth=0.00001):\n",
    "#     dice = 0\n",
    "#     for index in range(M):\n",
    "#         dice += dice_coef(y_true[:,:,:,index], y_pred[:,:,:,index], smooth)\n",
    "#     return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560e91c-1832-4f4a-8981-78af5c821843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgb_mask(img, num_classes=5):\n",
    "    img_array = np.array(img)\n",
    "    height, width, _ = img_array.shape\n",
    "    output = np.zeros((num_classes, height, width), dtype=np.uint8)\n",
    "    class_map = {\n",
    "        (0, 0, 0): 0,\n",
    "        (1, 1, 1): 1,\n",
    "        (2, 2, 2): 2,\n",
    "        (3, 3, 3): 3,\n",
    "        (4, 4, 4): 4,\n",
    "    }\n",
    "    for class_value, class_index in class_map.items():\n",
    "        mask = np.all(img_array == class_value, axis=-1)\n",
    "        output[class_index][mask] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766017-1a95-459c-a631-f70d598a596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image, mask, train):\n",
    "    if train:\n",
    "        image_transforms = A.Compose(\n",
    "            [\n",
    "                A.OneOf([\n",
    "                    A.ToGray(),\n",
    "                    A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=3, val_shift_limit=3),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.01, contrast_limit=0.01, brightness_by_max=False)\n",
    "                ], p=0.2),\n",
    "                ToTensorV2()\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        image_transforms = A.Compose(\n",
    "            [\n",
    "                ToTensorV2()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    image = image_transforms(image=image)\n",
    "    image = image['image']\n",
    "    image = image / 255\n",
    "    \n",
    "    mask = read_rgb_mask(mask)\n",
    "    mask = torch.Tensor(mask)\n",
    "    \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc06013-b860-4772-9039-a360bcf0408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mask(mask):\n",
    "    class_indices = torch.argmax(mask, dim=1)\n",
    "    return class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e8476-7938-4132-bf25-4336275bc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_rgb(output, color_map=None):\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            0: [0, 0, 0],\n",
    "            1: [201,0,118],\n",
    "            2: [34,97,38],\n",
    "            3: [41,134,204],\n",
    "            4: [116,71,0]\n",
    "        }\n",
    "    \n",
    "    output = output.detach().cpu().numpy()\n",
    "    output = np.argmax(output, axis=1)[0]\n",
    "    \n",
    "    height, width = output.shape\n",
    "    rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_idx, color in color_map.items():\n",
    "        rgb_image[output == class_idx] = color\n",
    "    \n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587166e-c575-4ad8-a5cc-7221f9d6e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_output(model, input_image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    \n",
    "    rgb_image = output_to_rgb(output)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda32f1-3087-4143-9766-97b4a32dc3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, preprocess_fn, train=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        mask = Image.open(self.target_paths[index])\n",
    "        \n",
    "        return self.preprocess_fn(np.array(image), np.array(mask, dtype=np.int8), train=self.train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f2149-d1d7-4d08-bbef-019da2ab4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "num_classes = 5  # Including background\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ed5e-add1-4153-bf27-f276fc5717bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths = glob.glob(os.path.join(\"landcover.ai.v1/train/image\", \"*.tif\"))\n",
    "train_img_paths = sorted(train_img_paths)\n",
    "\n",
    "train_mask_paths = glob.glob(os.path.join(\"landcover.ai.v1/train/label\", \"*.tif\"))\n",
    "train_mask_paths = sorted(train_mask_paths)\n",
    "\n",
    "val_img_paths = glob.glob(os.path.join(\"landcover.ai.v1/val/image\", \"*.tif\"))\n",
    "val_img_paths = sorted(val_img_paths)\n",
    "\n",
    "val_mask_paths = glob.glob(os.path.join(\"landcover.ai.v1/val/label\", \"*.tif\"))\n",
    "val_mask_paths = sorted(val_mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b67689-e34f-4a2b-adb8-fe2ea4f7e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    image_paths=train_img_paths,\n",
    "    target_paths=train_mask_paths,\n",
    "    preprocess_fn=preprocess_data,\n",
    "    train=True\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    image_paths=val_img_paths,\n",
    "    target_paths=val_mask_paths,\n",
    "    preprocess_fn=preprocess_data,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4f8b5-d6bb-4d95-b388-cafddb3068fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerUNet(in_channels=3, out_channels=num_classes).to(device)\n",
    "loss_fn = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a861fb-e509-44a8-b625-f072f40b67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"landcover_seg_model.pth\")\n",
    "    \n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    \n",
    "    # with torch.inference_mode():\n",
    "    #     for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "    #         images, masks = images.to(device), masks.to(device)\n",
    "    \n",
    "    #         outputs = model(images)\n",
    "    #         loss = loss_fn(outputs, masks)\n",
    "    \n",
    "    #         val_loss += loss.item()\n",
    "    \n",
    "    # val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # epoch_val_loss.append(val_loss)\n",
    "    \n",
    "    # print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6dad1-d343-44fa-b698-32912ce2a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"landcover_seg_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba338a2e-6b93-486a-851b-1f11afac7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MulticlassJaccardIndex(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9c0a1-9e7f-4ef1-9535-01c13e7690c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_iou = 0\n",
    "with torch.inference_mode():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        iou = metric(outputs, transform_mask(masks))\n",
    "        val_iou += iou\n",
    "print(f\"Validation IoU: {val_iou / len(val_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
