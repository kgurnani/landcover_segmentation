{"cells":[{"cell_type":"code","execution_count":null,"id":"vB4qserabZXf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33568,"status":"ok","timestamp":1720655311382,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"vB4qserabZXf","outputId":"0c673532-2c4c-4077-f49f-8c67cb249fb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"pE6AAvCoVYpP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60501,"status":"ok","timestamp":1720655371878,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"pE6AAvCoVYpP","outputId":"3966e7ec-6175-4311-8d14-b9ee6e90b15d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/868.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/868.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n","Successfully installed lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"id":"hKkY-CWOkYKE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57720,"status":"ok","timestamp":1720655429595,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"hKkY-CWOkYKE","outputId":"d29f3bbf-49df-4517-a751-fb2eb2104bf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Collecting torch\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Collecting triton==2.3.1 (from torch)\n","  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: triton, torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.3.0\n","    Uninstalling triton-2.3.0:\n","      Successfully uninstalled triton-2.3.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n","torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.3.1 triton-2.3.1\n"]}],"source":["!pip install -U torch"]},{"cell_type":"code","execution_count":null,"id":"ADpNRFfzRbN4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19900,"status":"ok","timestamp":1720655449486,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"ADpNRFfzRbN4","outputId":"012e4657-c423-4e30-85ab-6dd880eb0e38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mamba-ssm\n","  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.1)\n","Collecting ninja (from mamba-ssm)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops (from mamba-ssm)\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n","Building wheels for collected packages: mamba-ssm\n","  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323803485 sha256=cd8ee941b25398d90c135d3a180b5dc33e478c98ae4998b61749809beaff947a\n","  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n","Successfully built mamba-ssm\n","Installing collected packages: ninja, einops, mamba-ssm\n","Successfully installed einops-0.8.0 mamba-ssm-2.2.2 ninja-1.11.1.1\n"]}],"source":["!pip install mamba-ssm"]},{"cell_type":"code","execution_count":null,"id":"76d9ffe6-2106-4e0e-bac1-4984f88650b0","metadata":{"id":"76d9ffe6-2106-4e0e-bac1-4984f88650b0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from albumentations.pytorch import ToTensorV2\n","import numpy as np\n","from tqdm import tqdm\n","from torch.utils.data.dataset import Dataset\n","from PIL import Image\n","import glob\n","import os\n","import torch.nn.functional as F\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","from torchmetrics.classification import MulticlassJaccardIndex\n","import shutil\n","import cv2\n","from mamba_ssm import Mamba\n","import json\n","from einops.layers.torch import Rearrange\n","from torch import Tensor"]},{"cell_type":"code","execution_count":null,"id":"oP433x83Vt9H","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107709,"status":"ok","timestamp":1720655564739,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"oP433x83Vt9H","outputId":"29a54bfc-cff5-4e4e-d8b7-fb84927c9923"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-07-10 23:50:55--  https://landcover.ai.linuxpolska.com/download/landcover.ai.v1.zip\n","Resolving landcover.ai.linuxpolska.com (landcover.ai.linuxpolska.com)... 195.78.67.65\n","Connecting to landcover.ai.linuxpolska.com (landcover.ai.linuxpolska.com)|195.78.67.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1538212277 (1.4G) [application/zip]\n","Saving to: ‘landcover.ai.v1.zip’\n","\n","landcover.ai.v1.zip 100%[===================>]   1.43G  14.1MB/s    in 1m 47s  \n","\n","2024-07-10 23:52:43 (13.8 MB/s) - ‘landcover.ai.v1.zip’ saved [1538212277/1538212277]\n","\n"]}],"source":["!wget https://landcover.ai.linuxpolska.com/download/landcover.ai.v1.zip"]},{"cell_type":"code","execution_count":null,"id":"ezexmvBDWLlM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10819,"status":"ok","timestamp":1720655575553,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"ezexmvBDWLlM","outputId":"0d2cc2f1-d164-45bb-f3a8-db663d83fb0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  landcover.ai.v1.zip\n","   creating: landcover.ai.v1/images/\n","  inflating: landcover.ai.v1/images/M-33-48-A-c-4-4.tif  \n","  inflating: landcover.ai.v1/images/M-33-20-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/images/M-33-20-D-d-3-3.tif  \n","  inflating: landcover.ai.v1/images/M-33-32-B-b-4-4.tif  \n","  inflating: landcover.ai.v1/images/M-33-7-A-d-2-3.tif  \n","  inflating: landcover.ai.v1/images/M-33-7-A-d-3-2.tif  \n","  inflating: landcover.ai.v1/images/M-34-32-B-a-4-3.tif  \n","  inflating: landcover.ai.v1/images/M-34-32-B-b-1-3.tif  \n","  inflating: landcover.ai.v1/images/M-34-5-D-d-4-2.tif  \n","  inflating: landcover.ai.v1/images/M-34-51-C-b-2-1.tif  \n","  inflating: landcover.ai.v1/images/M-34-51-C-d-4-1.tif  \n","  inflating: landcover.ai.v1/images/M-34-55-B-b-4-1.tif  \n","  inflating: landcover.ai.v1/images/M-34-56-A-b-1-4.tif  \n","  inflating: landcover.ai.v1/images/M-34-6-A-d-2-2.tif  \n","  inflating: landcover.ai.v1/images/M-34-65-D-a-4-4.tif  \n","  inflating: landcover.ai.v1/images/M-34-65-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/images/M-34-65-D-d-4-1.tif  \n","  inflating: landcover.ai.v1/images/M-34-68-B-a-1-3.tif  \n","  inflating: landcover.ai.v1/images/M-34-77-B-c-2-3.tif  \n","  inflating: landcover.ai.v1/images/N-33-104-A-c-1-1.tif  \n","  inflating: landcover.ai.v1/images/N-33-119-C-c-3-3.tif  \n","  inflating: landcover.ai.v1/images/N-33-130-A-d-3-3.tif  \n","  inflating: landcover.ai.v1/images/N-33-130-A-d-4-4.tif  \n","  inflating: landcover.ai.v1/images/N-33-139-C-d-2-2.tif  \n","  inflating: landcover.ai.v1/images/N-33-139-C-d-2-4.tif  \n","  inflating: landcover.ai.v1/images/N-33-139-D-c-1-3.tif  \n","  inflating: landcover.ai.v1/images/N-33-60-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/images/N-33-60-D-d-1-2.tif  \n","  inflating: landcover.ai.v1/images/N-33-96-D-d-1-1.tif  \n","  inflating: landcover.ai.v1/images/N-34-106-A-b-3-4.tif  \n","  inflating: landcover.ai.v1/images/N-34-106-A-c-1-3.tif  \n","  inflating: landcover.ai.v1/images/N-34-140-A-b-3-2.tif  \n","  inflating: landcover.ai.v1/images/N-34-140-A-b-4-2.tif  \n","  inflating: landcover.ai.v1/images/N-34-140-A-d-3-4.tif  \n","  inflating: landcover.ai.v1/images/N-34-140-A-d-4-2.tif  \n","  inflating: landcover.ai.v1/images/N-34-61-B-a-1-1.tif  \n","  inflating: landcover.ai.v1/images/N-34-66-C-c-4-3.tif  \n","  inflating: landcover.ai.v1/images/N-34-77-A-b-1-4.tif  \n","  inflating: landcover.ai.v1/images/N-34-94-A-b-2-4.tif  \n","  inflating: landcover.ai.v1/images/N-34-97-C-b-1-2.tif  \n","  inflating: landcover.ai.v1/images/N-34-97-D-c-2-4.tif  \n","   creating: landcover.ai.v1/masks/\n","  inflating: landcover.ai.v1/masks/N-33-119-C-c-3-3.tif  \n","  inflating: landcover.ai.v1/masks/N-34-94-A-b-2-4.tif  \n","  inflating: landcover.ai.v1/masks/N-34-140-A-b-4-2.tif  \n","  inflating: landcover.ai.v1/masks/M-34-65-D-a-4-4.tif  \n","  inflating: landcover.ai.v1/masks/M-34-77-B-c-2-3.tif  \n","  inflating: landcover.ai.v1/masks/M-34-51-C-d-4-1.tif  \n","  inflating: landcover.ai.v1/masks/N-34-77-A-b-1-4.tif  \n","  inflating: landcover.ai.v1/masks/N-34-106-A-b-3-4.tif  \n","  inflating: landcover.ai.v1/masks/M-34-32-B-a-4-3.tif  \n","  inflating: landcover.ai.v1/masks/M-34-65-D-d-4-1.tif  \n","  inflating: landcover.ai.v1/masks/M-34-56-A-b-1-4.tif  \n","  inflating: landcover.ai.v1/masks/M-33-20-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/masks/N-34-140-A-b-3-2.tif  \n","  inflating: landcover.ai.v1/masks/N-33-60-D-d-1-2.tif  \n","  inflating: landcover.ai.v1/masks/N-33-139-C-d-2-4.tif  \n","  inflating: landcover.ai.v1/masks/N-33-60-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/masks/N-33-104-A-c-1-1.tif  \n","  inflating: landcover.ai.v1/masks/N-34-97-D-c-2-4.tif  \n","  inflating: landcover.ai.v1/masks/N-34-140-A-d-4-2.tif  \n","  inflating: landcover.ai.v1/masks/M-34-55-B-b-4-1.tif  \n","  inflating: landcover.ai.v1/masks/N-33-139-D-c-1-3.tif  \n","  inflating: landcover.ai.v1/masks/N-33-96-D-d-1-1.tif  \n","  inflating: landcover.ai.v1/masks/M-34-68-B-a-1-3.tif  \n","  inflating: landcover.ai.v1/masks/M-33-7-A-d-2-3.tif  \n","  inflating: landcover.ai.v1/masks/N-34-61-B-a-1-1.tif  \n","  inflating: landcover.ai.v1/masks/N-33-130-A-d-4-4.tif  \n","  inflating: landcover.ai.v1/masks/N-34-140-A-d-3-4.tif  \n","  inflating: landcover.ai.v1/masks/N-33-130-A-d-3-3.tif  \n","  inflating: landcover.ai.v1/masks/N-33-139-C-d-2-2.tif  \n","  inflating: landcover.ai.v1/masks/M-34-5-D-d-4-2.tif  \n","  inflating: landcover.ai.v1/masks/M-33-7-A-d-3-2.tif  \n","  inflating: landcover.ai.v1/masks/M-34-51-C-b-2-1.tif  \n","  inflating: landcover.ai.v1/masks/M-33-48-A-c-4-4.tif  \n","  inflating: landcover.ai.v1/masks/N-34-66-C-c-4-3.tif  \n","  inflating: landcover.ai.v1/masks/N-34-97-C-b-1-2.tif  \n","  inflating: landcover.ai.v1/masks/M-34-32-B-b-1-3.tif  \n","  inflating: landcover.ai.v1/masks/M-33-32-B-b-4-4.tif  \n","  inflating: landcover.ai.v1/masks/M-34-6-A-d-2-2.tif  \n","  inflating: landcover.ai.v1/masks/M-33-20-D-d-3-3.tif  \n","  inflating: landcover.ai.v1/masks/M-34-65-D-c-4-2.tif  \n","  inflating: landcover.ai.v1/masks/N-34-106-A-c-1-3.tif  \n","  inflating: landcover.ai.v1/split.py  \n","  inflating: landcover.ai.v1/test.txt  \n","  inflating: landcover.ai.v1/train.txt  \n","  inflating: landcover.ai.v1/val.txt  \n"]}],"source":["!unzip landcover.ai.v1.zip -d landcover.ai.v1"]},{"cell_type":"code","execution_count":null,"id":"m9mqVGafXk5q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274979,"status":"ok","timestamp":1720655850531,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"m9mqVGafXk5q","outputId":"e9c83065-6f70-4c4d-81e2-dcbd56ca55b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed M-33-20-D-c-4-2 1/41\n","Processed M-33-20-D-d-3-3 2/41\n","Processed M-33-32-B-b-4-4 3/41\n","Processed M-33-48-A-c-4-4 4/41\n","Processed M-33-7-A-d-2-3 5/41\n","Processed M-33-7-A-d-3-2 6/41\n","Processed M-34-32-B-a-4-3 7/41\n","Processed M-34-32-B-b-1-3 8/41\n","Processed M-34-5-D-d-4-2 9/41\n","Processed M-34-51-C-b-2-1 10/41\n","Processed M-34-51-C-d-4-1 11/41\n","Processed M-34-55-B-b-4-1 12/41\n","Processed M-34-56-A-b-1-4 13/41\n","Processed M-34-6-A-d-2-2 14/41\n","Processed M-34-65-D-a-4-4 15/41\n","Processed M-34-65-D-c-4-2 16/41\n","Processed M-34-65-D-d-4-1 17/41\n","Processed M-34-68-B-a-1-3 18/41\n","Processed M-34-77-B-c-2-3 19/41\n","Processed N-33-104-A-c-1-1 20/41\n","Processed N-33-119-C-c-3-3 21/41\n","Processed N-33-130-A-d-3-3 22/41\n","Processed N-33-130-A-d-4-4 23/41\n","Processed N-33-139-C-d-2-2 24/41\n","Processed N-33-139-C-d-2-4 25/41\n","Processed N-33-139-D-c-1-3 26/41\n","Processed N-33-60-D-c-4-2 27/41\n","Processed N-33-60-D-d-1-2 28/41\n","Processed N-33-96-D-d-1-1 29/41\n","Processed N-34-106-A-b-3-4 30/41\n","Processed N-34-106-A-c-1-3 31/41\n","Processed N-34-140-A-b-3-2 32/41\n","Processed N-34-140-A-b-4-2 33/41\n","Processed N-34-140-A-d-3-4 34/41\n","Processed N-34-140-A-d-4-2 35/41\n","Processed N-34-61-B-a-1-1 36/41\n","Processed N-34-66-C-c-4-3 37/41\n","Processed N-34-77-A-b-1-4 38/41\n","Processed N-34-94-A-b-2-4 39/41\n","Processed N-34-97-C-b-1-2 40/41\n","Processed N-34-97-D-c-2-4 41/41\n"]}],"source":["IMGS_DIR = \"./landcover.ai.v1/images\"\n","MASKS_DIR = \"./landcover.ai.v1/masks\"\n","OUTPUT_DIR = \"./landcover.ai.v1/output\"\n","\n","TARGET_SIZE = 512\n","\n","img_paths = glob.glob(os.path.join(IMGS_DIR, \"*.tif\"))\n","mask_paths = glob.glob(os.path.join(MASKS_DIR, \"*.tif\"))\n","\n","img_paths.sort()\n","mask_paths.sort()\n","\n","os.makedirs(OUTPUT_DIR)\n","for i, (img_path, mask_path) in enumerate(zip(img_paths, mask_paths)):\n","    img_filename = os.path.splitext(os.path.basename(img_path))[0]\n","    mask_filename = os.path.splitext(os.path.basename(mask_path))[0]\n","    img = cv2.imread(img_path)\n","    mask = cv2.imread(mask_path)\n","\n","    assert img_filename == mask_filename and img.shape[:2] == mask.shape[:2]\n","\n","    k = 0\n","    for y in range(0, img.shape[0], TARGET_SIZE):\n","        for x in range(0, img.shape[1], TARGET_SIZE):\n","            img_tile = img[y : y + TARGET_SIZE, x : x + TARGET_SIZE]\n","            mask_tile = mask[y : y + TARGET_SIZE, x : x + TARGET_SIZE]\n","\n","            if img_tile.shape[0] == TARGET_SIZE and img_tile.shape[1] == TARGET_SIZE:\n","                out_img_path = os.path.join(\n","                    OUTPUT_DIR, \"{}_{}.tif\".format(img_filename, k)\n","                )\n","                cv2.imwrite(out_img_path, img_tile)\n","\n","                out_mask_path = os.path.join(\n","                    OUTPUT_DIR, \"{}_{}_m.tif\".format(mask_filename, k)\n","                )\n","                cv2.imwrite(out_mask_path, mask_tile)\n","\n","            k += 1\n","\n","    print(\"Processed {} {}/{}\".format(img_filename, i + 1, len(img_paths)))"]},{"cell_type":"code","execution_count":null,"id":"GMVhfETqX3DI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9592,"status":"ok","timestamp":1720655860118,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"GMVhfETqX3DI","outputId":"4fd00cfb-641e-4e3f-b0cd-4b1c6e73e570"},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder 'landcover.ai.v1/train/image' created.\n","Folder 'landcover.ai.v1/train/label' created.\n","Folder 'landcover.ai.v1/val/image' created.\n","Folder 'landcover.ai.v1/val/label' created.\n","Folder 'landcover.ai.v1/test/image' created.\n","Folder 'landcover.ai.v1/test/label' created.\n","Train, Val, Test splits count\n","7470 1602 1602\n","Last item of each split\n","N-34-97-D-c-2-4_9 N-34-97-D-c-2-4_75 N-34-97-D-c-2-4_77\n","File copying completed.\n"]}],"source":["train_data_dir = \"landcover.ai.v1/train/image\"\n","train_label_dir = \"landcover.ai.v1/train/label\"\n","\n","val_data_dir = \"landcover.ai.v1/val/image\"\n","val_label_dir = \"landcover.ai.v1/val/label\"\n","\n","test_data_dir = \"landcover.ai.v1/test/image\"\n","test_label_dir = \"landcover.ai.v1/test/label\"\n","\n","paths = [\n","    train_data_dir,\n","    train_label_dir,\n","    val_data_dir,\n","    val_label_dir,\n","    test_data_dir,\n","    test_label_dir,\n","]\n","\n","for p in paths:\n","    if not os.path.exists(p):\n","        os.makedirs(p)\n","        print(f\"Folder '{p}' created.\")\n","    else:\n","        print(f\"Folder '{p}' already exists.\")\n","\n","\n","with open(\"landcover.ai.v1/train.txt\") as f:\n","    train_split = f.read().splitlines()\n","\n","with open(\"landcover.ai.v1/val.txt\") as f:\n","    val_split = f.read().splitlines()\n","\n","with open(\"landcover.ai.v1/test.txt\") as f:\n","    test_split = f.read().splitlines()\n","\n","print(\"Train, Val, Test splits count\")\n","print(len(train_split), len(val_split), len(test_split))\n","print(\"Last item of each split\")\n","print(train_split[-1], val_split[-1], test_split[-1])\n","\n","for i in train_split:\n","    source_file_img = os.path.join(\"landcover.ai.v1/output\", f\"{i}.tif\")\n","    source_file_label = os.path.join(\"landcover.ai.v1/output\", f\"{i}_m.tif\")\n","    destination_file_img = os.path.join(train_data_dir, f\"{i}.tif\")\n","    destination_file_label = os.path.join(train_label_dir, f\"{i}.tif\")\n","    if os.path.isfile(source_file_img) and not os.path.exists(destination_file_img):\n","        shutil.copy2(source_file_img, destination_file_img)\n","    if os.path.isfile(source_file_label) and not os.path.exists(destination_file_label):\n","        shutil.copy2(source_file_label, destination_file_label)\n","\n","for i in val_split:\n","    source_file_img = os.path.join(\"landcover.ai.v1/output\", f\"{i}.tif\")\n","    source_file_label = os.path.join(\"landcover.ai.v1/output\", f\"{i}_m.tif\")\n","    destination_file_img = os.path.join(val_data_dir, f\"{i}.tif\")\n","    destination_file_label = os.path.join(val_label_dir, f\"{i}.tif\")\n","    if os.path.isfile(source_file_img) and not os.path.exists(destination_file_img):\n","        shutil.copy2(source_file_img, destination_file_img)\n","    if os.path.isfile(source_file_label) and not os.path.exists(destination_file_label):\n","        shutil.copy2(source_file_label, destination_file_label)\n","\n","for i in test_split:\n","    source_file_img = os.path.join(\"landcover.ai.v1/output\", f\"{i}.tif\")\n","    source_file_label = os.path.join(\"landcover.ai.v1/output\", f\"{i}_m.tif\")\n","    destination_file_img = os.path.join(test_data_dir, f\"{i}.tif\")\n","    destination_file_label = os.path.join(test_label_dir, f\"{i}.tif\")\n","    if os.path.isfile(source_file_img) and not os.path.exists(destination_file_img):\n","        shutil.copy2(source_file_img, destination_file_img)\n","    if os.path.isfile(source_file_label) and not os.path.exists(destination_file_label):\n","        shutil.copy2(source_file_label, destination_file_label)\n","\n","print(\"File copying completed.\")"]},{"cell_type":"code","execution_count":null,"id":"a93df827-0e31-479c-8fa0-f506d92eee05","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":832,"status":"ok","timestamp":1720664259200,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"a93df827-0e31-479c-8fa0-f506d92eee05","outputId":"fb7f6d9b-087d-48d0-8a86-6262b19d7915"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.projection = nn.Sequential(\n","            nn.Conv2d(3, 512, kernel_size=16, stride=16),\n","            Rearrange('b e (h) (w) -> b (h w) e'),\n","        )\n","        self.positions = nn.Parameter(torch.randn((512 // 16) ** 2, 512))\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.projection(x)\n","        x += self.positions\n","        return x\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(512)\n","        self.attn = nn.MultiheadAttention(512, 8)\n","        self.norm2 = nn.LayerNorm(512)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(512, int(512 * 2)),\n","            nn.GELU(),\n","            nn.Linear(int(512 * 2), 512)\n","        )\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n","        x = x + self.mlp(self.norm2(x))\n","        return x"],"metadata":{"id":"fAXbMfuqeore"},"id":"fAXbMfuqeore","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ebcdc313-79b7-4192-bf50-e552608ff2a1","metadata":{"id":"ebcdc313-79b7-4192-bf50-e552608ff2a1"},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, int(dim * mlp_ratio)),\n","            nn.GELU(),\n","            nn.Dropout(drop),\n","            nn.Linear(int(dim * mlp_ratio), dim)\n","        )\n","        self.dropout = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = x + self.dropout(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n","        x = x + self.dropout(self.mlp(self.norm2(x)))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"ZDLnJaS-dyF9","metadata":{"id":"ZDLnJaS-dyF9"},"outputs":[],"source":["class MambaBlock(nn.Module):\n","    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.mamba = Mamba(d_model=dim, d_state=16, d_conv=4, expand=2)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, int(dim * mlp_ratio)),\n","            nn.GELU(),\n","            nn.Dropout(drop),\n","            nn.Linear(int(dim * mlp_ratio), dim)\n","        )\n","        self.dropout = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = x + self.dropout(self.mamba(self.norm1(x)))\n","        x = x + self.dropout(self.mlp(self.norm2(x)))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"IrEXxOo1equl","metadata":{"id":"IrEXxOo1equl"},"outputs":[],"source":["class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, embedding_dim, max_length=5000):\n","        super(FixedPositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_length, embedding_dim)\n","        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(\n","            torch.arange(0, embedding_dim, 2).float()\n","            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[: x.size(0), :]\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"t6psx6_tgug4","metadata":{"id":"t6psx6_tgug4"},"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 8, emb_size: int = 512, img_size = 512, drop=0.0):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        self.projection = nn.Sequential(\n","            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n","            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n","        )\n","        # self.position_encoding = nn.Parameter(torch.randn((img_size // patch_size) **2, emb_size))\n","        self.position_encoding = FixedPositionalEncoding(emb_size, (img_size // patch_size) **2)\n","        self.pe_dropout = nn.Dropout(drop)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.projection(x)\n","        # x += self.position_encoding\n","        x = self.position_encoding(x)\n","        x = self.pe_dropout(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"mKdRBvgOmsle","metadata":{"id":"mKdRBvgOmsle"},"outputs":[],"source":["class SETRMEncoder(nn.Module):\n","    def __init__(self, num_transformer_layers, mamba_to_transformer_ratio, emb_size=512, attn_heads=4, in_channels=3, patch_size=4, img_size=512, sample_v=1):\n","        super().__init__()\n","        self.num_transformer_layers = num_transformer_layers\n","        self.mamba_to_transformer_ratio = mamba_to_transformer_ratio\n","\n","        self.emb_size = emb_size\n","        self.attn_heads = attn_heads\n","        self.in_channels = in_channels\n","        self.patch_size = patch_size\n","        self.img_size = img_size\n","        self.sample_v = sample_v\n","\n","        self.final_dense_and_reshape = nn.Sequential(\n","            nn.Linear(emb_size, patch_size * patch_size * emb_size // (sample_v**2)),\n","            Rearrange(\"b (h w) (p1 p2 c) -> b c (h p1) (w p2)\", p1 = patch_size // sample_v, p2 = patch_size // sample_v, h = img_size // patch_size, w = img_size // patch_size, c = emb_size)\n","        )\n","\n","        self.transformer_layers = nn.ModuleList()\n","        self.mamba_layers = nn.ModuleList()\n","\n","        for i in range(num_transformer_layers):\n","            self.transformer_layers.append(TransformerBlock(dim=emb_size, num_heads=attn_heads))\n","\n","        for i in range(num_transformer_layers * mamba_to_transformer_ratio):\n","            self.mamba_layers.append(MambaBlock(dim=emb_size))\n","\n","        self.patch_embed = PatchEmbedding(in_channels=in_channels, patch_size=patch_size, emb_size=emb_size, img_size=img_size)\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","\n","        for i in range(len(self.transformer_layers)):\n","            x = self.transformer_layers[i](x)\n","            if len(self.mamba_layers) > 0:\n","                for j in range(i * self.mamba_to_transformer_ratio, (i * self.mamba_to_transformer_ratio) + self.mamba_to_transformer_ratio):\n","                    x = self.mamba_layers[j](x)\n","\n","        x = self.final_dense_and_reshape(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"xt1zTcRpiLph","metadata":{"id":"xt1zTcRpiLph"},"outputs":[],"source":["class SETRMDecoder(nn.Module):\n","    def __init__(self, in_channels, out_channels, features=[512, 256, 128, 64]):\n","        super().__init__()\n","\n","        # self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","\n","        self.decoder_1 = nn.Sequential(\n","            nn.Conv2d(in_channels, features[0], 3, padding=1),\n","            nn.BatchNorm2d(features[0]),\n","            nn.ReLU(inplace=True),\n","            # nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        )\n","        self.decoder_2 = nn.Sequential(\n","            nn.Conv2d(features[0], features[1], 3, padding=1),\n","            nn.BatchNorm2d(features[1]),\n","            nn.ReLU(inplace=True),\n","            # nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        )\n","        self.decoder_3 = nn.Sequential(\n","            nn.Conv2d(features[1], features[2], 3, padding=1),\n","            nn.BatchNorm2d(features[2]),\n","            nn.ReLU(inplace=True),\n","            # nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        )\n","        self.decoder_4 = nn.Sequential(\n","            nn.Conv2d(features[2], features[3], 3, padding=1),\n","            nn.BatchNorm2d(features[3]),\n","            nn.ReLU(inplace=True),\n","            # nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        )\n","\n","        self.final_out = nn.Conv2d(features[-1], out_channels, 3, padding=1)\n","\n","    def forward(self, x):\n","        x = self.decoder_1(x)\n","        # x = self.upsample(x)\n","        x = self.decoder_2(x)\n","        # x = self.upsample(x)\n","        x = self.decoder_3(x)\n","        x = self.decoder_4(x)\n","        x = self.final_out(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"xlp6gl3u8u4O","metadata":{"id":"xlp6gl3u8u4O"},"outputs":[],"source":["class SETRM(nn.Module):\n","    def __init__(self, num_transformer_layers=2, mamba_to_transformer_ratio=1, num_classes=5, emb_size=512, attn_heads=4, in_channels=3, patch_size=4, img_size=512, sample_v=1, features=[512, 256, 128, 64]):\n","        super().__init__()\n","        self.encoder = SETRMEncoder(num_transformer_layers, mamba_to_transformer_ratio, emb_size, attn_heads, in_channels, patch_size, img_size, sample_v)\n","        self.decoder = SETRMDecoder(emb_size, num_classes, features)\n","    def forward(self, x):\n","        encoded_x = self.encoder(x)\n","        decoded_x = self.decoder(encoded_x)\n","\n","        return decoded_x"]},{"cell_type":"code","execution_count":null,"id":"56766017-1a95-459c-a631-f70d598a596e","metadata":{"id":"56766017-1a95-459c-a631-f70d598a596e"},"outputs":[],"source":["def preprocess_data(image, mask, train):\n","    if train:\n","        image_transforms = A.Compose(\n","            [\n","                A.OneOf([\n","                    A.ToGray(),\n","                    A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=3, val_shift_limit=3),\n","                    A.RandomBrightnessContrast(brightness_limit=0.01, contrast_limit=0.01, brightness_by_max=False)\n","                ], p=0.2),\n","                ToTensorV2()\n","            ]\n","        )\n","    else:\n","        image_transforms = A.Compose(\n","            [\n","                ToTensorV2()\n","            ]\n","        )\n","\n","    image = image_transforms(image=image)\n","    image = image['image']\n","    image = image / 255\n","\n","    mask = torch.from_numpy(mask)\n","    mask = torch.permute(mask, (2, 0, 1))\n","    mask = mask[0]\n","\n","    return image, mask"]},{"cell_type":"code","execution_count":null,"id":"abda32f1-3087-4143-9766-97b4a32dc3c6","metadata":{"id":"abda32f1-3087-4143-9766-97b4a32dc3c6"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, image_paths, target_paths, preprocess_fn, train=False):\n","        self.image_paths = image_paths\n","        self.target_paths = target_paths\n","        self.preprocess_fn = preprocess_fn\n","        self.train = train\n","\n","    def __getitem__(self, index):\n","        image = Image.open(self.image_paths[index])\n","        mask = Image.open(self.target_paths[index])\n","\n","        return self.preprocess_fn(np.array(image), np.array(mask, dtype=np.int8), train=self.train)\n","\n","    def __len__(self):\n","        return len(self.image_paths)"]},{"cell_type":"code","execution_count":null,"id":"IZ2CJJjAcHAf","metadata":{"id":"IZ2CJJjAcHAf"},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    \"\"\"Dice Loss PyTorch\n","        Created by: Zhang Shuai\n","        Email: shuaizzz666@gmail.com\n","        dice_loss = 1 - 2*p*t / (p^2 + t^2). p and t represent predict and target.\n","    Args:\n","        weight: An array of shape [C,]\n","        predict: A float32 tensor of shape [N, C, *], for Semantic segmentation task is [N, C, H, W]\n","        target: A int64 tensor of shape [N, *], for Semantic segmentation task is [N, H, W]\n","    Return:\n","        diceloss\n","    \"\"\"\n","    def __init__(self, weight=None):\n","        super(DiceLoss, self).__init__()\n","        if weight is not None:\n","            weight = torch.Tensor(weight)\n","            self.weight = weight / torch.sum(weight) # Normalized weight\n","        self.smooth = 1e-5\n","\n","    def forward(self, predict, target):\n","        N, C = predict.size()[:2]\n","        predict = predict.view(N, C, -1) # (N, C, *)\n","        target = target.view(N, 1, -1) # (N, 1, *)\n","\n","        predict = F.softmax(predict, dim=1) # (N, C, *) ==> (N, C, *)\n","        ## convert target(N, 1, *) into one hot vector (N, C, *)\n","        target_onehot = torch.zeros(predict.size()).cuda()  # (N, 1, *) ==> (N, C, *)\n","        target_onehot.scatter_(1, target, 1)  # (N, C, *)\n","\n","        intersection = torch.sum(predict * target_onehot, dim=2)  # (N, C)\n","        union = torch.sum(predict.pow(2), dim=2) + torch.sum(target_onehot, dim=2)  # (N, C)\n","        ## p^2 + t^2 >= 2*p*t, target_onehot^2 == target_onehot\n","        dice_coef = (2 * intersection + self.smooth) / (union + self.smooth)  # (N, C)\n","\n","        if hasattr(self, 'weight'):\n","            if self.weight.type() != predict.type():\n","                self.weight = self.weight.type_as(predict)\n","                dice_coef = dice_coef * self.weight * C  # (N, C)\n","        dice_loss = 1 - torch.mean(dice_coef)  # 1\n","\n","        return dice_loss"]},{"cell_type":"code","execution_count":null,"id":"dwsNdjEocN-G","metadata":{"id":"dwsNdjEocN-G"},"outputs":[],"source":["class CEPlusDiceLoss(nn.Module):\n","    def __init__(self):\n","        super(CEPlusDiceLoss, self).__init__()\n","        self.dice_loss = DiceLoss()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","\n","    def forward(self, predict, target):\n","        dice = self.dice_loss(predict, target.to(torch.int64))\n","        ce = self.ce_loss(predict, target.long())\n","\n","        total = dice + ce\n","\n","        return total"]},{"cell_type":"code","execution_count":null,"id":"699f2149-d1d7-4d08-bbef-019da2ab4135","metadata":{"id":"699f2149-d1d7-4d08-bbef-019da2ab4135"},"outputs":[],"source":["batch_size = 4\n","num_epochs = 5\n","num_classes = 5  # Including background\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","continue_training = False"]},{"cell_type":"code","execution_count":null,"id":"e565ed5e-add1-4153-bf27-f276fc5717bd","metadata":{"id":"e565ed5e-add1-4153-bf27-f276fc5717bd"},"outputs":[],"source":["train_img_paths = glob.glob(os.path.join(\"landcover.ai.v1/train/image\", \"*.tif\"))\n","train_img_paths = sorted(train_img_paths)\n","\n","train_mask_paths = glob.glob(os.path.join(\"landcover.ai.v1/train/label\", \"*.tif\"))\n","train_mask_paths = sorted(train_mask_paths)\n","\n","val_img_paths = glob.glob(os.path.join(\"landcover.ai.v1/val/image\", \"*.tif\"))\n","val_img_paths = sorted(val_img_paths)\n","\n","val_mask_paths = glob.glob(os.path.join(\"landcover.ai.v1/val/label\", \"*.tif\"))\n","val_mask_paths = sorted(val_mask_paths)"]},{"cell_type":"code","execution_count":null,"id":"04b67689-e34f-4a2b-adb8-fe2ea4f7e4b5","metadata":{"id":"04b67689-e34f-4a2b-adb8-fe2ea4f7e4b5"},"outputs":[],"source":["train_dataset = CustomDataset(\n","    image_paths=train_img_paths,\n","    target_paths=train_mask_paths,\n","    preprocess_fn=preprocess_data,\n","    train=True\n",")\n","val_dataset = CustomDataset(\n","    image_paths=val_img_paths,\n","    target_paths=val_mask_paths,\n","    preprocess_fn=preprocess_data,\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True\n",")\n","val_loader = DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"id":"c1e4f8b5-d6bb-4d95-b388-cafddb3068fe","metadata":{"id":"c1e4f8b5-d6bb-4d95-b388-cafddb3068fe"},"outputs":[],"source":["# model = MambaTransformerUNet(in_channels=3, out_channels=num_classes).to(device)\n","model = SETRM(num_transformer_layers=4, mamba_to_transformer_ratio=0, emb_size=512, num_classes=num_classes, patch_size=8, attn_heads=16).to(device)\n","loss_fn = CEPlusDiceLoss()\n","optimizer = optim.AdamW(model.parameters())\n","metric = MulticlassJaccardIndex(num_classes=num_classes).to(device)"]},{"cell_type":"code","execution_count":null,"id":"962bwljVfO2q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1720664260754,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"},"user_tz":300},"id":"962bwljVfO2q","outputId":"16038989-c40d-4684-a615-a88c979020ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting from scratch\n"]}],"source":["if continue_training:\n","    print(\"Loading saved model\")\n","    model.load_state_dict(torch.load(\"/content/drive/MyDrive/landcover_seg_model.pth\"))\n","    optimizer.load_state_dict(torch.load(\"/content/drive/MyDrive/landcover_seg_opt.pth\"))\n","else:\n","    print(\"Starting from scratch\")"]},{"cell_type":"code","execution_count":null,"id":"89a861fb-e509-44a8-b625-f072f40b67be","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":979},"id":"89a861fb-e509-44a8-b625-f072f40b67be","executionInfo":{"status":"error","timestamp":1720673165648,"user_tz":300,"elapsed":41830,"user":{"displayName":"Kunal Gurnani","userId":"00052946301797653631"}},"outputId":"40aacea1-eed9-47c6-f136-51b0c6e829c0"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/5:\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1868/1868 [30:34<00:00,  1.02it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss: 1.6640\n","Train IoU: 0.2553\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 401/401 [02:25<00:00,  2.76it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Loss: 1.6236\n","Validation IoU: 0.2434\n","-----------------------------\n","Epoch 2/5:\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1868/1868 [30:40<00:00,  1.01it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss: 1.6058\n","Train IoU: 0.2701\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 401/401 [02:24<00:00,  2.77it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Loss: 1.5460\n","Validation IoU: 0.2827\n","-----------------------------\n","Epoch 3/5:\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1868/1868 [30:36<00:00,  1.02it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss: 1.5901\n","Train IoU: 0.2658\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 401/401 [02:24<00:00,  2.78it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Loss: 1.4858\n","Validation IoU: 0.2913\n","-----------------------------\n","Epoch 4/5:\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1868/1868 [30:38<00:00,  1.02it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Loss: 1.5050\n","Train IoU: 0.2738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Validation: 100%|██████████| 401/401 [02:26<00:00,  2.74it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Loss: 2.2692\n","Validation IoU: 0.1232\n","-----------------------------\n","Epoch 5/5:\n"]},{"output_type":"stream","name":"stderr","text":["Training:  53%|█████▎    | 981/1868 [16:08<14:35,  1.01it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-7893cdd11ad1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-325e8fb159c9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predict, target)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-abc8d47297a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predict, target)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (N, C, *) ==> (N, C, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m## convert target(N, 1, *) into one hot vector (N, C, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtarget_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, 1, *) ==> (N, C, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtarget_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, C, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epoch_train_loss = []\n","epoch_train_iou = []\n","epoch_val_loss = []\n","epoch_val_iou = []\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n","\n","    model.train()\n","    train_loss = 0\n","    train_iou = 0\n","\n","    for images, masks in tqdm(train_loader, desc=\"Training\"):\n","        images, masks = images.to(device), masks.to(device)\n","        outputs = model(images)\n","        loss = loss_fn(outputs, masks)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_iou += metric(outputs, masks)\n","\n","    train_loss = train_loss / len(train_loader)\n","    train_iou = train_iou / len(train_loader)\n","\n","    epoch_train_loss.append(train_loss)\n","    epoch_train_iou.append(train_iou)\n","\n","    print(f\"Train Loss: {train_loss:.4f}\")\n","    print(f\"Train IoU: {train_iou:.4f}\")\n","\n","    torch.save(model.state_dict(), \"/content/drive/MyDrive/landcover_seg_model.pth\")\n","    torch.save(optimizer.state_dict(), \"/content/drive/MyDrive/landcover_seg_opt.pth\")\n","\n","    model.eval()\n","    val_loss = 0\n","    val_iou = 0\n","\n","    with torch.inference_mode():\n","        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n","            images, masks = images.to(device), masks.to(device)\n","\n","            outputs = model(images)\n","            loss = loss_fn(outputs, masks)\n","\n","            val_loss += loss.item()\n","            val_iou += metric(outputs, masks)\n","\n","    val_loss = val_loss / len(val_loader)\n","    val_iou = val_iou / len(val_loader)\n","\n","    epoch_val_loss.append(val_loss)\n","    epoch_val_iou.append(val_iou)\n","\n","    print(f\"Validation Loss: {val_loss:.4f}\")\n","    print(f\"Validation IoU: {val_iou:.4f}\")\n","    print(\"-----------------------------\")"]},{"cell_type":"code","execution_count":null,"id":"tp1UZojmbHct","metadata":{"id":"tp1UZojmbHct"},"outputs":[],"source":["training_stats = {\n","    \"train_loss\": epoch_train_loss,\n","    \"train_iou\": [el.item() for el in epoch_train_iou],\n","    \"val_loss\": epoch_val_loss,\n","    \"val_iou\": [el.item() for el in epoch_val_iou]\n","}\n","\n","with open(\"/content/drive/MyDrive/mrp-train-stats.json\", \"w\") as outfile:\n","    json.dump(training_stats, outfile)"]},{"cell_type":"code","execution_count":null,"id":"cgpcI20j1bUO","metadata":{"id":"cgpcI20j1bUO"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}